{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetuning different pretrained BERTs on MedNLI dataset\n",
    "================\n",
    "\n",
    "Notebook adapted from https://gluon-nlp.mxnet.io/examples/sentence_embedding/bert.html\n",
    "\n",
    "MedNLI has 3 labels: entailment, contradiction, and neutral\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import io\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "from bert import data, model\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "# change `ctx` to `mx.cpu()` if no GPU is available.\n",
    "ctx = mx.gpu(0) \n",
    "#ctx = mx.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Pretrained Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = ['book_corpus_wiki_en_cased',\n",
    "               'biobert_v1.1_pubmed_cased',\n",
    "              'clinicalbert_uncased']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTModel(\n",
      "  (encoder): BERTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "    (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "    (transformer_cells): HybridSequential(\n",
      "      (0): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (1): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (2): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (3): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (4): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (5): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (6): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (7): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (8): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (9): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (10): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "      (11): BERTEncoderCell(\n",
      "        (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "        (attention_cell): MultiHeadAttentionCell(\n",
      "          (_base_cell): DotProductAttentionCell(\n",
      "            (_dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          )\n",
      "          (proj_query): Dense(768 -> 768, linear)\n",
      "          (proj_key): Dense(768 -> 768, linear)\n",
      "          (proj_value): Dense(768 -> 768, linear)\n",
      "        )\n",
      "        (proj): Dense(768 -> 768, linear)\n",
      "        (ffn): BERTPositionwiseFFN(\n",
      "          (ffn_1): Dense(768 -> 3072, linear)\n",
      "          (activation): GELU()\n",
      "          (ffn_2): Dense(3072 -> 768, linear)\n",
      "          (dropout_layer): Dropout(p = 0.1, axes=())\n",
      "          (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "        )\n",
      "        (layer_norm): BERTLayerNorm(eps=1e-12, axis=-1, center=True, scale=True, in_channels=768)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (word_embed): HybridSequential(\n",
      "    (0): Embedding(28996 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (token_type_embed): HybridSequential(\n",
      "    (0): Embedding(2 -> 768, float32)\n",
      "    (1): Dropout(p = 0.1, axes=())\n",
      "  )\n",
      "  (pooler): Dense(768 -> 768, Activation(tanh))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "bert_base, vocabulary = nlp.model.get_model('bert_12_768_12',\n",
    "                                             dataset_name=bert_models[1],\n",
    "                                             pretrained=True, ctx=ctx, use_pooler=True,\n",
    "                                             use_decoder=False, use_classifier=False)\n",
    "print(bert_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the model for SentencePair Classification\n",
    "by adding a BERTClassifier and a dense layer\n",
    "specify softmax for cross entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.classification.BERTClassifier(bert_base, num_classes=3, dropout=0.1)\n",
    "# only need to initialize the classifier layer.\n",
    "model.classifier.initialize(init=mx.init.Normal(0.02), ctx=ctx)\n",
    "model.hybridize(static_alloc=True)\n",
    "\n",
    "# softmax cross entropy loss for classification\n",
    "loss_function = mx.gluon.loss.SoftmaxCELoss()\n",
    "loss_function.hybridize(static_alloc=True)\n",
    "\n",
    "metric = mx.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform original MedNLI jsonl files to TSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(sentence_binary_parse):\n",
    "    sentence = sentence_binary_parse \\\n",
    "        .replace('(', ' ').replace(')', ' ') \\\n",
    "        .replace('-LRB-', '(').replace('-RRB-', ')') \\\n",
    "        .replace('-LSB-', '[').replace('-RSB-', ']')\n",
    "\n",
    "    tokens = sentence.split()\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def toTSV_mednli(filename,output):\n",
    "    \n",
    "\n",
    "    label_dict = {'entailment':'0',\n",
    "                 'contradiction':'1',\n",
    "                 'neutral':'2'}\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        with open(output,'w') as o:\n",
    "            for i,line in enumerate(f): #- add index for tracking which sentence\n",
    "                example = json.loads(line)\n",
    "\n",
    "                premise = ' '.join( i for i in get_tokens(example['sentence1_binary_parse']))\n",
    "                hypothesis = ' '.join( i for i in get_tokens(example['sentence2_binary_parse']))\n",
    "\n",
    "                label = label_dict[example.get('gold_label', None)]\n",
    "\n",
    "                o.write(f\"{label}\\t{premise}\\t{hypothesis}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "toTSV_mednli('mli_test_v1.jsonl','mli_test.tsv')\n",
    "toTSV_mednli('mli_train_v1.jsonl','mli_train.tsv')\n",
    "toTSV_mednli('mli_dev_v1.jsonl','mli_dev.tsv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\tIn the ED , initial VS revealed T 98.9 , HR 73 , BP 121/90 , RR 15 , O2 sat 98 % on RA .\tThe patient is hemodynamically stable\n",
      "\n",
      "1\tIn the ED , initial VS revealed T 98.9 , HR 73 , BP 121/90 , RR 15 , O2 sat 98 % on RA .\tThe patient is hemodynamically unstable .\n",
      "\n",
      "2\tIn the ED , initial VS revealed T 98.9 , HR 73 , BP 121/90 , RR 15 , O2 sat 98 % on RA .\tThe patient is in pain .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out some pairs\n",
    "test_tsv = io.open('mli_test.tsv', encoding='utf-8')\n",
    "for i in range(3):\n",
    "    print(test_tsv.readline())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset in TSV format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The patient was seen by his primary care physician after he had complained of a one-week history of dyspnea on exertion and jaw tightness .\n",
      "The patient has has coronary artery disease .\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Skip the first line, which is the schema\n",
    "num_discard_samples = 0\n",
    "# Split fields by tabs\n",
    "field_separator = nlp.data.Splitter('\\t')\n",
    "# Fields to select from the file\n",
    "field_indices = [1, 2, 0]\n",
    "data_train_raw = nlp.data.TSVDataset(filename='mli_train.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "\n",
    "\n",
    "data_train_raw = nlp.data.TSVDataset(filename='mli_train.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "\n",
    "data_dev_raw = nlp.data.TSVDataset(filename='mli_dev.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "\n",
    "data_test_raw = nlp.data.TSVDataset(filename='mli_test.tsv',\n",
    "                                 field_separator=field_separator,\n",
    "                                 num_discard_samples=num_discard_samples,\n",
    "                                 field_indices=field_indices)\n",
    "\n",
    "\n",
    "sample_id = 8\n",
    "# Sentence A\n",
    "print(data_train_raw[sample_id][0])\n",
    "# Sentence B\n",
    "print(data_train_raw[sample_id][1])\n",
    "# 0 means entailment, 1 contradiction, 2 neutral\n",
    "print(data_train_raw[sample_id][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data to BERT format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use\n",
    "`BERTDatasetTransform` to perform the following transformations:\n",
    "- tokenize\n",
    "the\n",
    "input sequences\n",
    "- insert [CLS] at the beginning\n",
    "- insert [SEP] between sentence\n",
    "A and sentence B, and at the end\n",
    "- generate segment ids to indicate whether\n",
    "a token belongs to the first sequence or the second sequence.\n",
    "- generate valid length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary used for tokenization = \n",
      "Vocab(size=28996, unk=\"[UNK]\", reserved=\"['[CLS]', '[SEP]', '[MASK]', '[PAD]']\")\n",
      "[PAD] token id = 0\n",
      "[CLS] token id = 101\n",
      "[SEP] token id = 102\n",
      "token ids = \n",
      "[  101  1103  5351  1108  1562  1118  1117  2425  1920  7454  1170  1119\n",
      "  1125 10790  1104   170  1141   118  1989  1607  1104   173  6834  1643\n",
      " 25362  1113  4252  7340  1988  1105  5139  3600  1757   119   102  1103\n",
      "  5351  1144  1144  1884 15789  1616 18593  3653   119   102     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "valid length = \n",
      "46\n",
      "segment ids = \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "label = \n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Use the vocabulary from pre-trained model for tokenization\n",
    "bert_tokenizer = nlp.data.BERTTokenizer(vocabulary, lower=True)\n",
    "\n",
    "# The maximum length of an input sequence\n",
    "max_len = 128\n",
    "\n",
    "# The labels for the two classes [(0 = not similar) or  (1 = similar)]\n",
    "all_labels = [\"0\", \"1\",\"2\"]\n",
    "\n",
    "# whether to transform the data as sentence pairs.\n",
    "# for single sentence classification, set pair=False\n",
    "# for regression task, set class_labels=None\n",
    "# for inference without label available, set has_label=False\n",
    "pair = True\n",
    "transform = data.transform.BERTDatasetTransform(bert_tokenizer, max_len,\n",
    "                                                class_labels=all_labels,\n",
    "                                                has_label=True,\n",
    "                                                pad=True,\n",
    "                                                pair=pair)\n",
    "data_train = data_train_raw.transform(transform)\n",
    "data_dev = data_dev_raw.transform(transform)\n",
    "data_test = data_test_raw.transform(transform)\n",
    "\n",
    "\n",
    "print('vocabulary used for tokenization = \\n%s'%vocabulary)\n",
    "print('%s token id = %s'%(vocabulary.padding_token, vocabulary[vocabulary.padding_token]))\n",
    "print('%s token id = %s'%(vocabulary.cls_token, vocabulary[vocabulary.cls_token]))\n",
    "print('%s token id = %s'%(vocabulary.sep_token, vocabulary[vocabulary.sep_token]))\n",
    "print('token ids = \\n%s'%data_train[sample_id][0])\n",
    "print('valid length = \\n%s'%data_train[sample_id][1])\n",
    "print('segment ids = \\n%s'%data_train[sample_id][2])\n",
    "print('label = \\n%s'%data_train[sample_id][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the model\n",
    "\n",
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with very few epochs. For demonstration, we use a fixed learning rate and\n",
    "skip the validation steps. For the optimizer, we leverage the ADAM optimizer which\n",
    "performs very well for NLP data and for BERT models in particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 16/706] loss=0.1349, lr=0.0000050, acc=0.965\n",
      "[Epoch 0 Batch 32/706] loss=0.1525, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 48/706] loss=0.0926, lr=0.0000050, acc=0.966\n",
      "[Epoch 0 Batch 64/706] loss=0.1593, lr=0.0000050, acc=0.963\n",
      "[Epoch 0 Batch 80/706] loss=0.1569, lr=0.0000050, acc=0.964\n",
      "[Epoch 0 Batch 96/706] loss=0.1101, lr=0.0000050, acc=0.964\n",
      "[Epoch 0 Batch 112/706] loss=0.2182, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 128/706] loss=0.1087, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 144/706] loss=0.1755, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 160/706] loss=0.1262, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 176/706] loss=0.2048, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 192/706] loss=0.0949, lr=0.0000050, acc=0.962\n",
      "[Epoch 0 Batch 208/706] loss=0.1179, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 224/706] loss=0.2011, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 240/706] loss=0.1660, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 256/706] loss=0.1397, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 272/706] loss=0.0664, lr=0.0000050, acc=0.961\n",
      "[Epoch 0 Batch 288/706] loss=0.2058, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 304/706] loss=0.1299, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 320/706] loss=0.2034, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 336/706] loss=0.1843, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 352/706] loss=0.1168, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 368/706] loss=0.1665, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 384/706] loss=0.0887, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 400/706] loss=0.1673, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 416/706] loss=0.2165, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 432/706] loss=0.2131, lr=0.0000050, acc=0.958\n",
      "[Epoch 0 Batch 448/706] loss=0.1180, lr=0.0000050, acc=0.958\n",
      "[Epoch 0 Batch 464/706] loss=0.1184, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 480/706] loss=0.1058, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 496/706] loss=0.1323, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 512/706] loss=0.1220, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 528/706] loss=0.1028, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 544/706] loss=0.1477, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 560/706] loss=0.1566, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 576/706] loss=0.0838, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 592/706] loss=0.1313, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 608/706] loss=0.2005, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 624/706] loss=0.0679, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 640/706] loss=0.1881, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 656/706] loss=0.1049, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 672/706] loss=0.2170, lr=0.0000050, acc=0.959\n",
      "[Epoch 0 Batch 688/706] loss=0.1141, lr=0.0000050, acc=0.960\n",
      "[Epoch 0 Batch 704/706] loss=0.1856, lr=0.0000050, acc=0.960\n",
      "[Epoch 1 Batch 16/706] loss=0.0848, lr=0.0000050, acc=0.984\n",
      "[Epoch 1 Batch 32/706] loss=0.1058, lr=0.0000050, acc=0.979\n",
      "[Epoch 1 Batch 48/706] loss=0.1236, lr=0.0000050, acc=0.974\n",
      "[Epoch 1 Batch 64/706] loss=0.0956, lr=0.0000050, acc=0.973\n",
      "[Epoch 1 Batch 80/706] loss=0.1103, lr=0.0000050, acc=0.972\n",
      "[Epoch 1 Batch 96/706] loss=0.0283, lr=0.0000050, acc=0.975\n",
      "[Epoch 1 Batch 112/706] loss=0.0728, lr=0.0000050, acc=0.975\n",
      "[Epoch 1 Batch 128/706] loss=0.0963, lr=0.0000050, acc=0.974\n",
      "[Epoch 1 Batch 144/706] loss=0.0562, lr=0.0000050, acc=0.975\n",
      "[Epoch 1 Batch 160/706] loss=0.1084, lr=0.0000050, acc=0.974\n",
      "[Epoch 1 Batch 176/706] loss=0.1505, lr=0.0000050, acc=0.973\n",
      "[Epoch 1 Batch 192/706] loss=0.0983, lr=0.0000050, acc=0.973\n",
      "[Epoch 1 Batch 208/706] loss=0.1499, lr=0.0000050, acc=0.972\n",
      "[Epoch 1 Batch 224/706] loss=0.1120, lr=0.0000050, acc=0.972\n",
      "[Epoch 1 Batch 240/706] loss=0.1250, lr=0.0000050, acc=0.972\n",
      "[Epoch 1 Batch 256/706] loss=0.1360, lr=0.0000050, acc=0.971\n",
      "[Epoch 1 Batch 272/706] loss=0.1540, lr=0.0000050, acc=0.971\n",
      "[Epoch 1 Batch 288/706] loss=0.1604, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 304/706] loss=0.1407, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 320/706] loss=0.1555, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 336/706] loss=0.0498, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 352/706] loss=0.0689, lr=0.0000050, acc=0.970\n",
      "[Epoch 1 Batch 368/706] loss=0.1082, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 384/706] loss=0.1736, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 400/706] loss=0.1447, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 416/706] loss=0.0719, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 432/706] loss=0.1627, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 448/706] loss=0.1402, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 464/706] loss=0.1097, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 480/706] loss=0.0547, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 496/706] loss=0.1300, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 512/706] loss=0.1227, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 528/706] loss=0.1024, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 544/706] loss=0.1092, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 560/706] loss=0.0964, lr=0.0000050, acc=0.969\n",
      "[Epoch 1 Batch 576/706] loss=0.1742, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 592/706] loss=0.1476, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 608/706] loss=0.0952, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 624/706] loss=0.1037, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 640/706] loss=0.1715, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 656/706] loss=0.1298, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 672/706] loss=0.1687, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 688/706] loss=0.1299, lr=0.0000050, acc=0.968\n",
      "[Epoch 1 Batch 704/706] loss=0.1115, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 16/706] loss=0.1363, lr=0.0000050, acc=0.960\n",
      "[Epoch 2 Batch 32/706] loss=0.1939, lr=0.0000050, acc=0.955\n",
      "[Epoch 2 Batch 48/706] loss=0.1031, lr=0.0000050, acc=0.962\n",
      "[Epoch 2 Batch 64/706] loss=0.0717, lr=0.0000050, acc=0.966\n",
      "[Epoch 2 Batch 80/706] loss=0.0824, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 96/706] loss=0.1651, lr=0.0000050, acc=0.967\n",
      "[Epoch 2 Batch 112/706] loss=0.1311, lr=0.0000050, acc=0.966\n",
      "[Epoch 2 Batch 128/706] loss=0.0517, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 144/706] loss=0.1198, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 160/706] loss=0.1215, lr=0.0000050, acc=0.967\n",
      "[Epoch 2 Batch 176/706] loss=0.1082, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 192/706] loss=0.0739, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 208/706] loss=0.1273, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 224/706] loss=0.1682, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 240/706] loss=0.0891, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 256/706] loss=0.1398, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 272/706] loss=0.1024, lr=0.0000050, acc=0.968\n",
      "[Epoch 2 Batch 288/706] loss=0.0568, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 304/706] loss=0.0745, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 320/706] loss=0.0956, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 336/706] loss=0.0470, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 352/706] loss=0.1625, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 368/706] loss=0.1271, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 384/706] loss=0.0876, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 400/706] loss=0.0615, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 416/706] loss=0.1182, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 432/706] loss=0.1144, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 448/706] loss=0.1806, lr=0.0000050, acc=0.969\n",
      "[Epoch 2 Batch 464/706] loss=0.0715, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 480/706] loss=0.1049, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 496/706] loss=0.1377, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 512/706] loss=0.0972, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 528/706] loss=0.0535, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 544/706] loss=0.0525, lr=0.0000050, acc=0.970\n",
      "[Epoch 2 Batch 560/706] loss=0.0783, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 576/706] loss=0.0763, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 592/706] loss=0.0496, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 608/706] loss=0.0119, lr=0.0000050, acc=0.972\n",
      "[Epoch 2 Batch 624/706] loss=0.0900, lr=0.0000050, acc=0.972\n",
      "[Epoch 2 Batch 640/706] loss=0.1793, lr=0.0000050, acc=0.972\n",
      "[Epoch 2 Batch 656/706] loss=0.1209, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 672/706] loss=0.1845, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 688/706] loss=0.1051, lr=0.0000050, acc=0.971\n",
      "[Epoch 2 Batch 704/706] loss=0.1372, lr=0.0000050, acc=0.971\n"
     ]
    }
   ],
   "source": [
    "# The hyperparameters\n",
    "batch_size = 16 # adjust this according to the GPU memory, too high batch size, out of memory will occur\n",
    "lr = 5e-6\n",
    "\n",
    "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_train],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_train, batch_sampler=train_sampler)\n",
    "\n",
    "trainer = mx.gluon.Trainer(model.collect_params(), 'adam',\n",
    "                           {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 16\n",
    "num_epochs = 3\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    y_hat = []\n",
    "    y_label = []\n",
    "    \n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "            ls = loss_function(out, label).mean()\n",
    "\n",
    "        # And backwards computation\n",
    "        ls.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(params, 1)\n",
    "        trainer.update(1)\n",
    "\n",
    "        step_loss += ls.asscalar()\n",
    "        metric.update([label], [out])\n",
    "        \n",
    "#         #copy to cpu\n",
    "        \n",
    "     \n",
    "#         y_pred = out.copyto(mx.cpu()).asnumpy()\n",
    "#         y_pred = [np.argmax(i) for i in y_pred]\n",
    "#         y_hat.extend(y_pred)\n",
    "#         y_true = label.copyto(mx.cpu()).asnumpy().T.flatten()\n",
    "#         y_label.extend(y_true)\n",
    "        \n",
    "      \n",
    "    \n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "    \n",
    "#             print('\\n')\n",
    "#             print('ACCCCC')\n",
    "#             print(np.mean(np.array(y_hat) == np.array(y_label)))\n",
    "            step_loss = 0\n",
    "            \n",
    "#     print('\\n\\n\\n\\n')\n",
    "#     print(f'{epoch_id} accuracy {np.mean(np.array(y_hat) == np.array(y_label))}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 16/91] loss=0.0000, lr=0.0000050, acc=0.825\n",
      "[Epoch 0 Batch 32/91] loss=0.0000, lr=0.0000050, acc=0.814\n",
      "[Epoch 0 Batch 48/91] loss=0.0000, lr=0.0000050, acc=0.812\n",
      "[Epoch 0 Batch 64/91] loss=0.0000, lr=0.0000050, acc=0.828\n",
      "[Epoch 0 Batch 80/91] loss=0.0000, lr=0.0000050, acc=0.827\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DEV accuracy 0.8272401433691756\n"
     ]
    }
   ],
   "source": [
    "# GET DEV Accuracy\n",
    "batch_size = 16 # adjust this according to the GPU memory, too high batch size, out of memory will occur\n",
    "lr = 5e-6\n",
    "\n",
    "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_dev],\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False)\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_dev, batch_sampler=train_sampler)\n",
    "\n",
    "# trainer = mx.gluon.Trainer(model.collect_params(), 'adam',\n",
    "#                            {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 16\n",
    "num_epochs = 1\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    y_hat = []\n",
    "    y_label = []\n",
    "    \n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "           \n",
    "        metric.update([label], [out])\n",
    "        \n",
    "        #copy to cpu\n",
    "        \n",
    "     \n",
    "        y_pred = out.copyto(mx.cpu()).asnumpy()\n",
    "        y_pred = [np.argmax(i) for i in y_pred]\n",
    "        y_hat.extend(y_pred)\n",
    "        y_true = label.copyto(mx.cpu()).asnumpy().T.flatten()\n",
    "        y_label.extend(y_true)\n",
    "        \n",
    "      \n",
    "    \n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "        \n",
    "            \n",
    "    print('\\n\\n\\n\\n')\n",
    "    print(f'DEV accuracy {np.mean(np.array(y_hat) == np.array(y_label))}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 15/95] loss=0.0000, lr=0.0000050, acc=0.858\n",
      "[Epoch 0 Batch 30/95] loss=0.0000, lr=0.0000050, acc=0.818\n",
      "[Epoch 0 Batch 45/95] loss=0.0000, lr=0.0000050, acc=0.815\n",
      "[Epoch 0 Batch 60/95] loss=0.0000, lr=0.0000050, acc=0.818\n",
      "[Epoch 0 Batch 75/95] loss=0.0000, lr=0.0000050, acc=0.816\n",
      "[Epoch 0 Batch 90/95] loss=0.0000, lr=0.0000050, acc=0.826\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "TEST accuracy 0.8241912798874824\n"
     ]
    }
   ],
   "source": [
    "# GET TEST Accuracy\n",
    "batch_size = 15 # adjust this according to the GPU memory, too high batch size, out of memory will occur\n",
    "lr = 5e-6\n",
    "\n",
    "# The FixedBucketSampler and the DataLoader for making the mini-batches\n",
    "# train_sampler = nlp.data.FixedBucketSampler(lengths=[int(item[1]) for item in data_test],\n",
    "#                                             batch_size=batch_size,\n",
    "#                                             shuffle=False)\n",
    "# bert_dataloader = mx.gluon.data.DataLoader(data_dev, batch_sampler=train_sampler)\n",
    "\n",
    "\n",
    "bert_dataloader = mx.gluon.data.DataLoader(data_test, batch_size=15)\n",
    "# trainer = mx.gluon.Trainer(model.collect_params(), 'adam',\n",
    "#                            {'learning_rate': lr, 'epsilon': 1e-9})\n",
    "\n",
    "# Collect all differentiable parameters\n",
    "# `grad_req == 'null'` indicates no gradients are calculated (e.g. constant parameters)\n",
    "# The gradients for these params are clipped later\n",
    "params = [p for p in model.collect_params().values() if p.grad_req != 'null']\n",
    "grad_clip = 1\n",
    "\n",
    "# Training the model with only three epochs\n",
    "log_interval = 15\n",
    "num_epochs = 1\n",
    "for epoch_id in range(num_epochs):\n",
    "    metric.reset()\n",
    "    y_hat = []\n",
    "    y_label = []\n",
    "    \n",
    "    step_loss = 0\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(bert_dataloader):\n",
    "        with mx.autograd.record():\n",
    "\n",
    "            # Load the data to the GPU\n",
    "            \n",
    "            token_ids = token_ids.as_in_context(ctx)\n",
    "            valid_length = valid_length.as_in_context(ctx)\n",
    "            segment_ids = segment_ids.as_in_context(ctx)\n",
    "            label = label.as_in_context(ctx)\n",
    "\n",
    "            # Forward computation\n",
    "            out = model(token_ids, segment_ids, valid_length.astype('float32'))\n",
    "           \n",
    "        metric.update([label], [out])\n",
    "        \n",
    "        #copy to cpu\n",
    "        \n",
    "     \n",
    "        y_pred = out.copyto(mx.cpu()).asnumpy()\n",
    "        y_pred = [np.argmax(i) for i in y_pred]\n",
    "        y_hat.extend(y_pred)\n",
    "        y_true = label.copyto(mx.cpu()).asnumpy().T.flatten()\n",
    "        y_label.extend(y_true)\n",
    "        \n",
    "      \n",
    "      \n",
    "    \n",
    "        # Printing vital information\n",
    "        if (batch_id + 1) % (log_interval) == 0:\n",
    "            print('[Epoch {} Batch {}/{}] loss={:.4f}, lr={:.7f}, acc={:.3f}'\n",
    "                         .format(epoch_id, batch_id + 1, len(bert_dataloader),\n",
    "                                 step_loss / log_interval,\n",
    "                                 trainer.learning_rate, metric.get()[1]))\n",
    "        \n",
    "            \n",
    "    print('\\n\\n\\n\\n')\n",
    "    print(f'TEST accuracy {np.mean(np.array(y_hat) == np.array(y_label))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1394</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1396</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1403</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1404</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1407</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1411</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1417</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1418</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1419</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1420</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1421</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1422 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_hat  label\n",
       "0         0      0\n",
       "1         1      1\n",
       "2         2      2\n",
       "3         0      0\n",
       "4         1      1\n",
       "5         1      2\n",
       "6         2      0\n",
       "7         1      1\n",
       "8         2      2\n",
       "9         0      0\n",
       "10        1      1\n",
       "11        2      2\n",
       "12        2      0\n",
       "13        1      1\n",
       "14        2      2\n",
       "15        0      0\n",
       "16        1      1\n",
       "17        2      2\n",
       "18        0      0\n",
       "19        1      1\n",
       "20        2      2\n",
       "21        1      0\n",
       "22        1      1\n",
       "23        2      2\n",
       "24        0      0\n",
       "25        1      1\n",
       "26        2      2\n",
       "27        0      0\n",
       "28        1      1\n",
       "29        2      2\n",
       "...     ...    ...\n",
       "1392      0      0\n",
       "1393      1      1\n",
       "1394      2      2\n",
       "1395      0      0\n",
       "1396      1      1\n",
       "1397      2      2\n",
       "1398      1      0\n",
       "1399      1      1\n",
       "1400      1      2\n",
       "1401      0      0\n",
       "1402      1      1\n",
       "1403      1      2\n",
       "1404      0      0\n",
       "1405      1      1\n",
       "1406      2      2\n",
       "1407      1      0\n",
       "1408      1      1\n",
       "1409      2      2\n",
       "1410      2      0\n",
       "1411      1      1\n",
       "1412      2      2\n",
       "1413      0      0\n",
       "1414      1      1\n",
       "1415      2      2\n",
       "1416      0      0\n",
       "1417      1      1\n",
       "1418      2      2\n",
       "1419      2      0\n",
       "1420      1      1\n",
       "1421      2      2\n",
       "\n",
       "[1422 rows x 2 columns]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'y_hat':y_hat,'label':y_label})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1422, 3)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('mli_test.tsv',sep='\\t',header=None)\n",
    "df2.columns = ['class','premise','hypothesis']\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(df2,how='inner',left_index=True, right_index=True).to_csv('bioberttest.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df.merge(df2,how='inner',left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8241912798874824"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(df_final.y_hat == df_final['class'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
